{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 495,
      "metadata": {
        "id": "yqteAzW5BSgo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda')"
      ],
      "metadata": {
        "id": "2v9Y3GuJnfuS"
      },
      "execution_count": 496,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWu183rLnhp1",
        "outputId": "8bd5a088-9cc6-48c3-b747-0d9a8d6916c9"
      },
      "execution_count": 497,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/price_dataset/train_data.csv', delimiter=\",\")"
      ],
      "metadata": {
        "id": "ffCkyB6ZnnCx"
      },
      "execution_count": 498,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "QWuYr57GnrfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"over100k\"] = df[\"SalePrice\"] > 1e5\n",
        "df[\"over350k\"] = df[\"SalePrice\"] > 3.5e5\n",
        "df.drop(columns = \"SalePrice\", inplace=True)"
      ],
      "metadata": {
        "id": "5BYwfesV10UZ"
      },
      "execution_count": 500,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "Hb4Xkzkj5IBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "apartment_category_columns = [\n",
        "    \"HallwayType\",\n",
        "    \"HeatingType\",\n",
        "    \"AptManageType\"\n",
        "]\n",
        "\n",
        "transport_category_columns = [\n",
        "    \"TimeToBusStop\",\n",
        "    \"TimeToSubway\",\n",
        "    \"SubwayStation\"\n",
        "]\n",
        "\n",
        "apartment_category_values = pd.get_dummies(\n",
        "    df[apartment_category_columns]\n",
        ")\n",
        "\n",
        "transport_category_values = pd.get_dummies(\n",
        "    df[transport_category_columns]\n",
        ")"
      ],
      "metadata": {
        "id": "V8juu8UI5Js7"
      },
      "execution_count": 502,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "apartment_category_values"
      ],
      "metadata": {
        "id": "Ye-VLAQV9ivn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transport_category_values"
      ],
      "metadata": {
        "id": "PA1IYSj99lHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_100k_values = df[\"over100k\"]\n",
        "target_350k_values = df[\"over350k\"]\n",
        "target_all_values = target_100k_values.astype(int) + target_350k_values.astype(int)\n",
        "target_columns = [\n",
        "    \"over100k\",\n",
        "    \"over350k\"\n",
        "]"
      ],
      "metadata": {
        "id": "XUtedTyd9m8F"
      },
      "execution_count": 505,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns = apartment_category_columns + transport_category_columns + target_columns, inplace=True)"
      ],
      "metadata": {
        "id": "WtZzcCPm-MbU"
      },
      "execution_count": 506,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_SIZE = 0.2\n",
        "test_indicies = np.random.rand(len(df)) < TEST_SIZE"
      ],
      "metadata": {
        "id": "YOaZFA3Z-tbj"
      },
      "execution_count": 507,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_numerical = torch.from_numpy(df.values[~test_indicies]).float()\n",
        "test_numerical = torch.from_numpy(df.values[test_indicies]).float()\n",
        "\n",
        "train_apartment_category = torch.from_numpy(apartment_category_values.values[~test_indicies]).float()\n",
        "test_apartment_category = torch.from_numpy(apartment_category_values.values[test_indicies]).float()\n",
        "\n",
        "train_transport_category = torch.from_numpy(transport_category_values.values[~test_indicies]).float()\n",
        "test_transport_category = torch.from_numpy(transport_category_values.values[test_indicies]).float()\n",
        "\n",
        "train_target100k = torch.from_numpy(target_100k_values.values[~test_indicies]).float()\n",
        "test_target100k = torch.from_numpy(target_100k_values.values[test_indicies]).float()\n",
        "\n",
        "train_target350k = torch.from_numpy(target_350k_values.values[~test_indicies]).float()\n",
        "test_target350k = torch.from_numpy(target_350k_values.values[test_indicies]).float()\n",
        "test_target_all = torch.from_numpy(target_all_values.values[test_indicies]).float()\n"
      ],
      "metadata": {
        "id": "y859P7EDEi90"
      },
      "execution_count": 508,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_target100k_tensor = data.TensorDataset(train_numerical, train_apartment_category, train_transport_category, train_target100k)\n",
        "test_target100k_tensor = data.TensorDataset(test_numerical, test_apartment_category, test_transport_category, test_target100k)\n",
        "\n",
        "train_target350k_tensor = data.TensorDataset(train_numerical, train_apartment_category, train_transport_category, train_target350k)\n",
        "test_target350k_tensor = data.TensorDataset(test_numerical, test_apartment_category, test_transport_category, test_target350k)\n",
        "\n",
        "test_target_all_tensor = data.TensorDataset(test_numerical, test_apartment_category, test_transport_category, test_target_all)"
      ],
      "metadata": {
        "id": "Ls2h9VQntxhX"
      },
      "execution_count": 509,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_size = df.shape[1]\n",
        "apartment_category_size = apartment_category_values.shape[1]\n",
        "transport_category_size = transport_category_values.shape[1]"
      ],
      "metadata": {
        "id": "F5m5oSvsIWQ1"
      },
      "execution_count": 510,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\" Numerical size: {numerical_size}\\n Apartment category size: {apartment_category_size}\\n Transport category size: {transport_category_size}\")"
      ],
      "metadata": {
        "id": "mqyKm7bTIxL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DROPOUT = 0.2\n",
        "LR = 1e-3\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = 128\n",
        "WEIGHT_DECAY = 1e-4"
      ],
      "metadata": {
        "id": "TZF5T9qImAtc"
      },
      "execution_count": 512,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.apartment_emb_layer = nn.Linear(apartment_category_size, apartment_category_size)\n",
        "        self.transport_emb_layer = nn.Linear(transport_category_size, transport_category_size)\n",
        "        self.act_emb = nn.Tanh()\n",
        "        self.linear1 = nn.Linear(apartment_category_size + transport_category_size + numerical_size, 128)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.drop1 = nn.Dropout(DROPOUT)\n",
        "        self.linear2 = nn.Linear(128, 64)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.act2 = nn.ReLU()\n",
        "        self.drop2 = nn.Dropout(DROPOUT)\n",
        "        self.linear3 = nn.Linear(64, 32)\n",
        "        self.bn3 = nn.BatchNorm1d(32)\n",
        "        self.act3 = nn.ReLU()\n",
        "        self.drop3 = nn.Dropout(DROPOUT)\n",
        "        self.linear4 = nn.Linear(32, 1)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "    def forward(self, x, apartment_emb, transport_emb):\n",
        "        apartment_emb = self.apartment_emb_layer(apartment_emb)\n",
        "        apartment_emb = self.act_emb(apartment_emb)\n",
        "        transport_emb = self.transport_emb_layer(transport_emb)\n",
        "        transport_emb = self.act_emb(transport_emb)\n",
        "        x = torch.cat([x, apartment_emb, transport_emb], dim=1)\n",
        "        x = self.routine(self.linear1, self.bn1, self.act1, self.drop1, x)\n",
        "        x = self.routine(self.linear2, self.bn2, self.act2, self.drop2, x)\n",
        "        x = self.routine(self.linear3, self.bn3, self.act3, self.drop3, x)\n",
        "        x = self.linear4(x)\n",
        "        return x\n",
        "\n",
        "    def routine(self, linear_layer, batch_norm, activation, dropout, x):\n",
        "        x = linear_layer(x)\n",
        "        x = batch_norm(x)\n",
        "        x = activation(x)\n",
        "        x = dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ULWCcKy4Hzi_"
      },
      "execution_count": 513,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(model, data_loader):\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "      for x, apartment, transport, label in data_loader:\n",
        "          x = x.to(device)\n",
        "          label = label.to(device)\n",
        "          apartment = apartment.to(device)\n",
        "          transport = transport.to(device)\n",
        "          preds = model(x, apartment, transport).squeeze()\n",
        "          preds = preds > 0\n",
        "          correct += preds.eq(label).sum().item()\n",
        "          total += x.shape[0]\n",
        "\n",
        "  return correct / total"
      ],
      "metadata": {
        "id": "mXVVofpw628t"
      },
      "execution_count": 514,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_100k_loader = data.DataLoader(train_target100k_tensor, batch_size=BATCH_SIZE, shuffle=True)\n",
        "train_350k_loader = data.DataLoader(train_target350k_tensor, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "JeBHbb-Rtp0B"
      },
      "execution_count": 515,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_100k_loader = data.DataLoader(test_target100k_tensor, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_350k_loader = data.DataLoader(test_target350k_tensor, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "6iuQW5396sd3"
      },
      "execution_count": 516,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_all_loader = data.DataLoader(test_target_all_tensor, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "41ZCMQFULiFM"
      },
      "execution_count": 517,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model100k = NeuralNetwork()\n",
        "model100k.to(device)"
      ],
      "metadata": {
        "id": "fWKRxOvFOGRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=model100k.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "loss_module = nn.BCEWithLogitsLoss(pos_weight=(len(train_target100k)-sum(train_target100k))/sum(train_target100k))"
      ],
      "metadata": {
        "id": "F-qWoS9s39GL"
      },
      "execution_count": 519,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model100k.train()\n",
        "loss_list = []\n",
        "for epoch in range(EPOCHS):\n",
        "  epoch_loss = []\n",
        "  for x, apartment, transport, y in train_100k_loader:\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "      apartment = apartment.to(device)\n",
        "      transport = transport.to(device)\n",
        "      outputs = model100k(x, apartment, transport)\n",
        "      loss = loss_module(outputs.squeeze(dim=1), y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      epoch_loss.append(loss.item())\n",
        "  mean_loss = np.array(epoch_loss).mean()\n",
        "  loss_list.append(mean_loss)\n",
        "  if epoch % 10 == 0:\n",
        "    print(f\"Epoch: {epoch}, loss={mean_loss:.3}\")"
      ],
      "metadata": {
        "id": "bZSqb23E43Zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss_list)\n",
        "plt.title(\"Loss for 100k model\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "35j6GH5l_PSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Accuracy (100k): {get_accuracy(model100k, test_100k_loader)}\")"
      ],
      "metadata": {
        "id": "i8Ft0ZC3CmkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model350k = NeuralNetwork()\n",
        "model350k.to(device)"
      ],
      "metadata": {
        "id": "igEH4L0cDizN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=model350k.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "loss_module = nn.BCEWithLogitsLoss(pos_weight=(len(train_target350k)-sum(train_target350k))/sum(train_target350k))"
      ],
      "metadata": {
        "id": "xz36SmuFDoNF"
      },
      "execution_count": 524,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model350k.train()\n",
        "loss_list = []\n",
        "for epoch in range(EPOCHS):\n",
        "  epoch_loss = []\n",
        "  for x, apartment, transport, y in train_350k_loader:\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "      apartment = apartment.to(device)\n",
        "      transport = transport.to(device)\n",
        "      outputs = model350k(x, apartment, transport)\n",
        "      loss = loss_module(outputs.squeeze(dim=1), y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      epoch_loss.append(loss.item())\n",
        "  mean_loss = np.array(epoch_loss).mean()\n",
        "  loss_list.append(mean_loss)\n",
        "  if epoch % 10 == 0:\n",
        "    print(f\"Epoch: {epoch}, loss={mean_loss:.3}\")"
      ],
      "metadata": {
        "id": "SqRxPEUODq3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss_list)\n",
        "plt.title(\"Loss for 350k model\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "be4VdP6aDtvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Accuracy (350k): {get_accuracy(model350k, test_350k_loader)}\")"
      ],
      "metadata": {
        "id": "58qHses4Dys0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model100k.eval()\n",
        "model350k.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for x, apartment, transport, label in test_all_loader:\n",
        "        x = x.to(device)\n",
        "        apartment = apartment.to(device)\n",
        "        transport = transport.to(device)\n",
        "        label = pd.DataFrame(label.cpu().detach().numpy())\n",
        "\n",
        "        preds_100k = model100k(x, apartment, transport).squeeze()\n",
        "        preds_100k = pd.DataFrame(preds_100k.cpu().detach().numpy())\n",
        "\n",
        "        preds_350k = model350k(x, apartment, transport).squeeze() \n",
        "        preds_350k = pd.DataFrame(preds_350k.cpu().detach().numpy())\n",
        "        \n",
        "        preds = (preds_100k > 0).astype(int) + (preds_350k > 0).astype(int)\n",
        "        correct += (preds == label).sum().item()\n",
        "        total += x.shape[0]\n",
        "\n",
        "print(f\"Accuracy: {correct / total}\")"
      ],
      "metadata": {
        "id": "HNCcoG6JLSUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_df = pd.read_csv('/content/drive/MyDrive/price_dataset/test_data.csv', delimiter=\",\")"
      ],
      "metadata": {
        "id": "dheaMEPeptMr"
      },
      "execution_count": 529,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_df.head()"
      ],
      "metadata": {
        "id": "sVouf6Cqvx8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_transportation_categorical_values = pd.get_dummies(\n",
        "      eval_df[transport_category_columns]\n",
        ")\n",
        "\n",
        "eval_apartment_categorical_values = pd.get_dummies(\n",
        "      eval_df[apartment_category_columns]\n",
        ")"
      ],
      "metadata": {
        "id": "vEr3az6wqGIB"
      },
      "execution_count": 531,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_df.drop(columns=transport_category_columns + \n",
        "                apartment_category_columns, inplace=True)"
      ],
      "metadata": {
        "id": "K-uVhij7qXeL"
      },
      "execution_count": 532,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_numerical_data = torch.from_numpy(\n",
        "    eval_df.values\n",
        ").float()\n",
        "\n",
        "eval_transportation_categorical_data = torch.from_numpy(\n",
        "    eval_transportation_categorical_values.values\n",
        ").float()\n",
        "\n",
        "eval_apartment_categorical_data = torch.from_numpy(\n",
        "    eval_apartment_categorical_values.values\n",
        ").float()"
      ],
      "metadata": {
        "id": "YGAC6dYgqcxL"
      },
      "execution_count": 533,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = data.TensorDataset(eval_numerical_data,eval_apartment_categorical_data,\n",
        "                                  eval_transportation_categorical_data)"
      ],
      "metadata": {
        "id": "Xl--88X6qhat"
      },
      "execution_count": 534,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_data_loader = torch.utils.data.DataLoader(eval_dataset, batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "id": "2AJU9kvkqmOp"
      },
      "execution_count": 535,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "count = {0:0,1:0, 2:0}\n",
        "with open('wyniki.csv', 'w') as f:\n",
        "    writer = csv.writer(f)\n",
        "    model350k.eval()\n",
        "    model100k.eval()\n",
        "    with torch.no_grad():\n",
        "        for x,apartment, transport in eval_data_loader:\n",
        "            x, apartment, transport = (\n",
        "                x.to(device), apartment.to(device), \n",
        "                transport.to(device)\n",
        "            )\n",
        "            output100k = model100k(x, apartment, transport).squeeze(dim=1)\n",
        "            output100k = pd.DataFrame(output100k.cpu().detach().numpy())\n",
        "\n",
        "            output350k = model350k(x, apartment, transport).squeeze(dim=1)\n",
        "            output350k = pd.DataFrame(output350k.cpu().detach().numpy())\n",
        "            pred = (output100k > 0).astype(int) + (output350k > 0).astype(int)\n",
        "            pred = pred.values[0][0]\n",
        "            count[pred] += 1\n",
        "            writer.writerow([pred])\n",
        "print(count)"
      ],
      "metadata": {
        "id": "LAfYOGtYqn-q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}